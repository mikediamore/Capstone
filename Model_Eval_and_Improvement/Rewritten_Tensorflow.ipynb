{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work in Progress:\n",
    "\n",
    "1.) Transfer model to full tensorflow implementation. Find a way to get maxout layer to work, also having trouble with latest tensorflow 1.4.0 release and using gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: Michael Di Amore\n",
    "\"\"\"\n",
    "\n",
    "#Had to use python 2 for TF 1.4.0 was having problems with my virutal env\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import Query as query\n",
    "import quandl \n",
    "import numpy as np\n",
    "# np.random.seed(12345) # Set seed\n",
    "\n",
    "import pandas as pd\n",
    "from imblearn.combine import SMOTEENN \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pdb\n",
    "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score,recall_score,precision_score\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "#  Load Data. If you don't want to / can't run the query.\n",
    "# Basically this is all data from 2007-01-01 to 2017-10-06\n",
    "load_data = True\n",
    "if load_data == True:\n",
    "    print ('Loading Data...')\n",
    "    df_2001_2006 = pd.read_csv('Gdelt_events_20000101_20061231.csv')\n",
    "    df_2001_2006 = df_2001_2006.set_index('sqldate',drop=True).sort_index()\n",
    "    df_2007_2013 = pd.read_csv('Gdelt_events_20070101_20131231.csv')\n",
    "    df_2007_2013 = df_2007_2013.set_index('sqldate',drop=True).sort_index()\n",
    "    df_2014_01 = pd.read_csv('Gdelt_events_20140101_20140531.csv')\n",
    "    df_2014_01  = df_2014_01 .set_index('sqldate',drop=True).sort_index()\n",
    "    df_2014_02 = pd.read_csv('Gdelt_events_20140601_20141231.csv')\n",
    "    df_2014_02  = df_2014_02 .set_index('sqldate',drop=True).sort_index()\n",
    "    \n",
    "    df_2015 = pd.read_csv('Gdelt_events_20150101_20151231.csv')\n",
    "    df_2015 = df_2015.set_index('sqldate',drop=True).sort_index()\n",
    "    df_2016_2017 = pd.read_csv('Gdelt_events_20160101_20171006.csv')\n",
    "    df_2016_2017 = df_2016_2017.set_index('sqldate',drop=True).sort_index()\n",
    "    gdelt_df = pd.concat([df_2001_2006,df_2007_2013,df_2014_01,df_2014_02,df_2015,df_2016_2017])\n",
    "    \n",
    "    del df_2001_2006,df_2007_2013,df_2014_01,df_2014_02,df_2015,df_2016_2017\n",
    "    \n",
    "####\n",
    "# Set Params\n",
    "lookback_window = int(np.round(252/2))\n",
    "number_stdev = 2\n",
    "daily_security_vol_target = .15/np.sqrt(252) #.15 is the 10 year SPY volalility found here\n",
    "#http://performance.morningstar.com/funds/etf/ratings-risk.action?t=SPY&region=usa&culture=en-US\n",
    "####\n",
    "\n",
    "proj_id = 'capstone-v0'\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2017-10-06'\n",
    "ticker = '^GSPC'\n",
    "my_query = query.query_tool(proj_id,start_date,end_date,ticker)\n",
    "sql_query = \"\"\"\n",
    "            SELECT Actor1Name, GoldsteinScale,NumMentions,sourceurl,\n",
    "            sqldate, avgtone, numarticles, numsources,  \n",
    "            FROM [gdelt-bq:full.events] \n",
    "            WHERE sqldate > 20010101 and sqldate <= 20061231  and \n",
    "            Actor1Code like '%BUS%'and\n",
    "            Actor1Geo_CountryCode like \"%US%\"\n",
    "            \"\"\"\n",
    "if load_data == False:\n",
    "    print ('Querying Gdelt...')\n",
    "    my_query.query_gdelt(sql_query)\n",
    "    my_query.gdelt_df = my_query.gdelt_df.set_index('sqldate',drop=True).sort_index()\n",
    "    df = my_query.gdelt_df.copy(True)\n",
    "\n",
    "#Creating Labels. i.e. if change in spx_return is x standard deviations\n",
    "security_prices = my_query.query_yahoo()\n",
    "security_return = np.log(security_prices['Adj Close']).diff() #log Return\n",
    "security_vol = security_return.rolling(window=lookback_window).std().dropna()\n",
    "security_mean = security_return.rolling(window=lookback_window).mean().dropna()\n",
    "security_vol.name = 'Volatility'\n",
    "security_return = security_return.loc[security_vol.index] #First entry is NAN because of return\n",
    "# day_over_day_diff = np.abs(security_return.diff())#can subtract because of log returns\n",
    "# event_idx = [(np.abs(security_mean)+(security_vol * 3.5)) < np.abs(security_return)]\n",
    "event_idx = [((security_mean + security_vol * number_stdev) < security_return) | ((security_mean - security_vol *number_stdev) > security_return) ]\n",
    "event_idx = np.array(event_idx).astype(int).flatten()\n",
    "event_idx = pd.Series(event_idx,index=security_return.index)\n",
    "print ('Data Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quandl Loaded\n"
     ]
    }
   ],
   "source": [
    "api_key =  'reqa36mksyfgx9BR6r88'\n",
    "wti_co = my_query.query_quandl(\"FRED/DCOILWTICO\",api_key)\n",
    "unemploy = my_query.query_quandl(\"FRED/UNEMPLOY\",api_key)\n",
    "m1v = my_query.query_quandl(\"FRED/M1V\",api_key)\n",
    "m2v = my_query.query_quandl(\"FRED/M2V\",api_key)\n",
    "stressindex = my_query.query_quandl(\"FRED/STLFSI\", api_key)\n",
    "dff = my_query.query_quandl(\"FRED/DFF\",api_key)\n",
    "my_query.set_ticker('^VIX')\n",
    "vix = my_query.query_yahoo()\n",
    "vix = vix['Adj Close']\n",
    "vix.name = 'VIX'\n",
    "print ('Quandl Loaded')\n",
    "\n",
    "#Use GLD as proxy for gold. Better to use the futures but don't have good continuous contracts\n",
    "# my_query.set_ticker('GLD')\n",
    "# gld = my_query.query_yahoo()\n",
    "# gld = gld['Adj Close']\n",
    "\n",
    "#Use Continuous Future of Front month gold contract as gold\n",
    "# gold = my_query.query_quandl(\"SCF/CME_GC1_OR\",api_key)\n",
    "# gold = gold['Settle']\n",
    "# #Use Continuous Future of front month  crude as oil\n",
    "# oil = my_query.query_quandl(\"SCF/ICE_B1_OB\",api_key)\n",
    "# oil = oil['Settle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quandl_others = pd.concat([wti_co,unemploy,m1v,m2v,stressindex,vix,dff],axis=1)\n",
    "quandl_others.columns = ['wit_co','unemploy','m1v','m2v','slsi','vix','dff']\n",
    "quandl_others.ffill(inplace=True)\n",
    "quandl_others.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_ts(series,lookback=lookback_window):\n",
    "    rolling_mean = series.rolling(window=lookback).mean()\n",
    "    rolling_std = series.rolling(window=lookback).std() + .10**3\n",
    "    normalized = (series-rolling_mean)/rolling_std\n",
    "    normalized = normalized.dropna()\n",
    "    return(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling using custom scaler\n"
     ]
    }
   ],
   "source": [
    "#Collapse numerical data into x,y pairs by taking means\n",
    "collapsed = gdelt_df.groupby(by=gdelt_df.index).mean()\n",
    "\n",
    "#Shift data so as only to use yesterday's news for tomorrow's prediction\n",
    "#i.e. we shift forward, using yesterday data as today\n",
    "collapsed_shifted = collapsed.shift(1)\n",
    "quandl_shifted = quandl_others.shift(1)\n",
    "\n",
    "#Scale the data in such a way that we aren't looking forward into the feature\n",
    "print ('Scaling using custom scaler')\n",
    "collapsed_shifted = collapsed_shifted.apply(normalize_ts)\n",
    "quandl_shifted = quandl_shifted.apply(normalize_ts)\n",
    "\n",
    "collapsed_shifted.index = pd.to_datetime(collapsed_shifted.index,format='%Y%m%d')\n",
    "collapsed_shifted = collapsed_shifted.loc[security_vol.index].dropna()\n",
    "collapsed_shifted = pd.concat([collapsed_shifted,quandl_shifted],axis=1)\n",
    "event_idx = event_idx.dropna()\n",
    "collapsed_shifted = collapsed_shifted.loc[event_idx.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train test split...\n"
     ]
    }
   ],
   "source": [
    "print ('Creating train test split...')\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(collapsed_shifted,event_idx,stratify=None,test_size=.20,shuffle=False)\n",
    "X_train,X_val,Y_train,Y_val = train_test_split(X_train,Y_train,test_size=.20,stratify=None,shuffle=False)\n",
    "\n",
    "#Creating Copies of Data Frames these will be useful later for debugging\n",
    "X_train_df = X_train.copy(True)\n",
    "Y_train_df = Y_train.copy(True)\n",
    "Y_val_df = Y_val.copy(True)\n",
    "X_train  = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train before smote\n",
      "2780\n",
      "Number of Positives before smote\n",
      "166\n",
      "Performing Oversampling/Undersampling...\n",
      "Length of X_train after smote\n",
      "4536\n",
      "Number of Positive samples after smote\n",
      "2574\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pca = PCA(whiten=True)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "X_val = pca.transform(X_val)\n",
    "\n",
    "print ('Length of X_train before smote')\n",
    "print (len(X_train))\n",
    "print ('Number of Positives before smote')\n",
    "print (Y_train.sum())\n",
    "\n",
    "\n",
    "print ('Performing Oversampling/Undersampling...')\n",
    "s = SMOTEENN()\n",
    "X_train,Y_train= s.fit_sample(X_train,Y_train)\n",
    "\n",
    "print ('Length of X_train after smote')\n",
    "print (len(X_train))\n",
    "\n",
    "print ('Number of Positive samples after smote')\n",
    "print (Y_train.sum())\n",
    "\n",
    "# print ('Proportion after smote {}'.format(sum(Y_train)/len(Y_train))\n",
    "\n",
    "X_train = X_train.reshape(-1,X_train.shape[1],1)\n",
    "X_test = X_test.reshape(-1,X_test.shape[1],1)\n",
    "X_val = X_val.reshape(-1,X_val.shape[1],1)\n",
    "print ('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Version of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.keras as k\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = X_train.shape[1]\n",
    "Y_val = Y_val\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32)\n",
    "Y_train = Y_train.astype(np.int32)\n",
    "Y_val = Y_val.astype(np.int32)\n",
    "features = {'x':X_train}\n",
    "drop_rate = .55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Had to use a separate function couldn't get TF GPU to work with TF 1.4.0\n",
    "#Taken from \n",
    "\n",
    "def max_out(inputs, num_units, axis=None):\n",
    "    shape = inputs.get_shape().as_list()\n",
    "    if shape[0] is None:\n",
    "        shape[0] = -1\n",
    "    if axis is None:  # Assume that channel is the last dimension\n",
    "        axis = -1\n",
    "    num_channels = shape[axis]\n",
    "    if num_channels % num_units:\n",
    "        raise ValueError('number of features({}) is not '\n",
    "                         'a multiple of num_units({})'.format(num_channels, num_units))\n",
    "    shape[axis] = num_units\n",
    "    shape += [num_channels // num_units]\n",
    "    outputs = tf.reduce_max(tf.reshape(inputs, shape), -1, keep_dims=False)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpcm98o7mo\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_model_dir': '/tmp/tmpcm98o7mo', '_save_summary_steps': 100, '_session_config': None, '_tf_random_seed': 1, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "number of features(128) is not a multiple of num_units(1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d69c8fb98ceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mmy_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/src/anaconda3/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps)\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/src/anaconda3/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m       estimator_spec = self._call_model_fn(features, labels,\n\u001b[0;32m--> 560\u001b[0;31m                                            model_fn_lib.ModeKeys.TRAIN)\n\u001b[0m\u001b[1;32m    561\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOSSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m       all_hooks.extend([\n",
      "\u001b[0;32m/home/ubuntu/src/anaconda3/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     model_fn_results = self._model_fn(\n\u001b[0;32m--> 545\u001b[0;31m         features=features, labels=labels, **kwargs)\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimatorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-d69c8fb98ceb>\u001b[0m in \u001b[0;36mtf_model\u001b[0;34m(features, labels, mode)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#         flat_shape = flatten.get_shape().as_list()[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mlayer7\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmax_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mlayer7\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m#     layer7 = tf.layers.dropout(inputs=layer7,training=is_training,rate=drop_rate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-cbd3576d1022>\u001b[0m in \u001b[0;36mmax_out\u001b[0;34m(inputs, num_units, axis)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_channels\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         raise ValueError('number of features({}) is not '\n\u001b[0;32m---> 13\u001b[0;31m                          'a multiple of num_units({})'.format(num_channels, num_units))\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_channels\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: number of features(128) is not a multiple of num_units(1024)"
     ]
    }
   ],
   "source": [
    "def tf_model(features,labels,mode):\n",
    "    with tf.device('/gpu:0'):\n",
    "        if tf.estimator.ModeKeys.TRAIN == 'train':\n",
    "            is_training=True\n",
    "        else:\n",
    "            is_training = False\n",
    "        layer1 =  tf.layers.conv1d(inputs=features['x'],name='x',filters=512,kernel_size=5,padding='same',\n",
    "                                                       activation=tf.nn.relu,kernel_initializer=k.initializers.he_normal())\n",
    "        layer1 =  tf.contrib.layers.batch_norm(layer1, is_training=is_training, decay=0.999)\n",
    "        layer1 = tf.layers.dropout(inputs=layer1,training=is_training,rate=drop_rate)\n",
    "\n",
    "\n",
    "        layer2 =  tf.layers.conv1d(layer1,filters=256,kernel_size=5,padding='same',\n",
    "                                   activation=tf.nn.relu,kernel_initializer=k.initializers.he_normal())\n",
    "        layer2 =  tf.contrib.layers.batch_norm(layer2, is_training=is_training, decay=0.999)\n",
    "        layer2 = tf.layers.dropout(inputs=layer2,training=is_training,rate=drop_rate)\n",
    "\n",
    "\n",
    "        layer3 =  tf.layers.conv1d(layer2,filters=128,kernel_size=5,padding='same',activation=tf.nn.relu,kernel_initializer=k.initializers.he_normal())\n",
    "        layer3 =  tf.contrib.layers.batch_norm(layer3, is_training=is_training, decay=0.999)\n",
    "        layer3 = tf.layers.dropout(inputs=layer1,training=is_training,rate=drop_rate)\n",
    "\n",
    "\n",
    "        layer4 =  tf.layers.conv1d(layer3,filters=128,kernel_size=3,padding='same',activation=tf.nn.relu,kernel_initializer=k.initializers.he_normal())\n",
    "        layer4 =  tf.contrib.layers.batch_norm(layer4, is_training=is_training, decay=0.999)\n",
    "        layer4 = tf.layers.dropout(inputs=layer4,training=is_training,rate=drop_rate)\n",
    "\n",
    "\n",
    "        layer5 =  tf.layers.conv1d(layer4,filters=128,kernel_size=3,padding='same',activation=tf.nn.relu,kernel_initializer=k.initializers.he_normal())\n",
    "        layer5 =  tf.contrib.layers.batch_norm(layer5, is_training=is_training, decay=0.999)\n",
    "        layer5 = tf.layers.dropout(inputs=layer5,training=is_training,rate=drop_rate)\n",
    "\n",
    "\n",
    "        layer6 =  tf.layers.conv1d(layer5,filters=128,kernel_size=3,padding='same',activation=tf.nn.relu,kernel_initializer=k.initializers.he_normal())\n",
    "        layer6 =  tf.contrib.layers.batch_norm(layer6, is_training=is_training, decay=0.999)\n",
    "        layer6 = tf.layers.dropout(inputs=layer6,training=is_training,rate=drop_rate)\n",
    "\n",
    "        flatten = tf.contrib.layers.flatten(inputs=layer6)\n",
    "        flat_shape = flatten.get_shape().as_list()[1]\n",
    "        \n",
    "        layer7 =  max_out(flatten,flat_shape-1024)\n",
    "        layer7 =  tf.contrib.layers.batch_norm(layer7, is_training=is_training, decay=0.999)\n",
    "    #     layer7 = tf.layers.dropout(inputs=layer7,training=is_training,rate=drop_rate)\n",
    "\n",
    "        layer8 =  max_out(layer7,512)\n",
    "        layer8 =  tf.contrib.layers.batch_norm(layer8, is_training=is_training, decay=0.999)\n",
    "    #     layer8 = tf.layers.dropout(inputs=layer8,training=is_training,rate=drop_rate)\n",
    "\n",
    "        layer9 =  max_out(layer8,256)\n",
    "        layer9 =  tf.contrib.layers.batch_norm(layer9, is_training=is_training, decay=0.999)\n",
    "    #     layer9 = tf.layers.dropout(inputs=layer9,training=is_training,rate=drop_rate)\n",
    "\n",
    "        logits = tf.layers.dense(layer9,units=2)\n",
    "        \n",
    "\n",
    "        predictions = {\n",
    "          \"classes\": tf.argmax(input=logits, axis=1),\n",
    "          \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "            }\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "          # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "        ohc =  tf.one_hot(indices=tf.cast(labels, tf.int32), depth=2)\n",
    "        loss = tf.losses.softmax_cross_entropy(onehot_labels=ohc, logits=logits)\n",
    "\n",
    "        # Configure the Training Op (for TRAIN mode)\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "            train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "        # Add evaluation metrics (for EVAL mode)\n",
    "        eval_metric_ops = {\n",
    "          \"accuracy\": tf.metrics.accuracy(\n",
    "              labels=labels, predictions=predictions[\"classes\"])}\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "          mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "my_classifier = tf.estimator.Estimator(\n",
    "    model_fn=tf_model)\n",
    "    \n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": np.array(X_train)},\n",
    "      y=np.array(Y_train),\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_val},\n",
    "    y=Y_val,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "\n",
    "my_classifier.train(input_fn=train_input_fn, steps=2000)\n",
    "eval_results = my_classifier.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "predictions = list(my_classifier.predict(input_fn=eval_input_fn))\n",
    "predicted_classes = [p[\"classes\"] for p in predictions]\n",
    "print(predicted_classes)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Standard Deviations: 2\n",
      " Accuracy: 0.42589928057553955\n",
      " F1: 0.111358574610245\n",
      " RoC: 0.5437648895095704\n",
      " Precision: 0.06067961165048544\n",
      " Recall: 0.6756756756756757\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(Y_val,predicted_classes)\n",
    "f1 = f1_score(Y_val,predicted_classes)\n",
    "roc = roc_auc_score(Y_val,predicted_classes)\n",
    "prec = precision_score(Y_val,predicted_classes)\n",
    "recall= recall_score(Y_val,predicted_classes)\n",
    "\n",
    "\n",
    "print (' Standard Deviations: {}'.format(number_stdev))\n",
    "print (' Accuracy: {}'.format(acc))\n",
    "print (' F1: {}'.format(f1))\n",
    "print (' RoC: {}'.format(roc))\n",
    "print (' Precision: {}'.format(prec))\n",
    "print (' Recall: {}'.format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
